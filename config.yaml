samples:
    # specifies expected location of split reads
    fastq: [data/split_read.1.fq.gz, data/split_read.2.fq.gz]
    # path to directory with the below lanes fastqs
    fq_path: "../fastq"
    # list of lanes to use. Should be directories within the fq_path directory
    lanes: [V350114043_L04]
    # sample or id to use for generating targets
    id: "data"
    # Chromosomes to evaluate for fragment length and phasing
    # Leave empty if there are too many chromosomes to specify or you're unsure
    # This will force snakemake to evaluate all chromosomes
    chroms: [chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY]
modules:
    # Should snakemake do stlfr specific analyses, false if the library isn't of linked reads
    stLFR: True
    # Should a VCF be generated
    variant_calling: False
    # Should VCF results be benchmarked against the files specified under benchmark
    benchmarking: False
    # Should longhap and hapcut be run and evaluated
    phasing: False
    # You'll probably never run this
    mate_pair_analysis: False
    # or this
    read_overlap_analysis: False
    # or this
    duplicate_plot: False
    # output files necessary for UMI analysis
    umi_analysis: False
params:
    
    ########### path in SJ ###########
    # ref_fa: "/research/rv-02/home/eanderson/CGI_WGS_Pipeline/Data_and_Tools/data/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"
    # toolsdir: "/research/rv-02/home/eanderson/CGI_WGS_Pipeline/Data_and_Tools"
    # src_dir: "/home/ycai/hm/dev/pipeline/dev/CGI_WGS_nonstandard_Pipeline/"
    # # path to gatk install, if you change you may have to modify flags in rules
    # gatk_install: "/home/eanderson/gatk-4.1.2.0/gatk"
    # calc_frag_python: "/home/ycai/anaconda3/envs/General3/bin/python"
    # # Path to dbSNP, update for hs37d5. Leave blank if there's no appropriate version of dnsnp
    # dbsnp_path: "/research/rv-02/home/eanderson/CGI_WGS_Pipeline/Data_and_Tools/data/dbsnp_138.hg38.chrfix.vcf.gz"
    # #GC Bias index for calculations
    # gc_bias_index: "/research/rv-02/home/eanderson/CGI_WGS_Pipeline/Data_and_Tools/data/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa.500.gc"
    # # rtg for SNP accuracy
    # rtg_install: "/research/rv-02/home/qmao/Scripts/rtg-tools-3.8.4/rtg"
    gcbias_python: "/home/ycai/anaconda3/envs/gcbias/bin/python"
    ########### ###########

    ########### path in SZ ###########
    ref_fa: "/home/ycai/ycai/pipeline/CGI_WGS_Pipeline/Data_and_Tools/data/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"
    toolsdir: "/home/ycai/ycai/pipeline/CGI_WGS_Pipeline/Data_and_Tools/"
    src_dir: "/hwfssz8/MGI_CG_SZ/USER/luozhenyu/software/git_pipeline/dev/CGI_WGS_nonstandard_Pipeline/"
    # path to gatk install, if you change you may have to modify flags in rules
    gatk_install: "/home/ycai/ycai/pipeline/CGI_WGS_Pipeline/Data_and_Tools/tools/gatk-4.1.2.0/gatk"
    calc_frag_python: "/hwfssz8/MGI_CG_SZ/USER/luozhenyu/anaconda3/envs/General3/bin/python"
    # Path to dbSNP, update for hs37d5. Leave blank if there's no appropriate version of dnsnp
    dbsnp_path: "/home/ycai/ycai/pipeline/CGI_WGS_Pipeline/Data_and_Tools/data/dbsnp_138.hg38.chrfix.vcf.gz"
    #GC Bias index for calculations
    gc_bias_index: "/home/ycai/ycai/pipeline/CGI_WGS_Pipeline/Data_and_Tools/data/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa.500.gc"
    # rtg for SNP accuracy
    rtg_install: "/hwfssz8/MGI_CG_SZ/USER/luozhenyu/software/rtg-tools-3.8.4/rtg"
    # Path to the python version Qing used for gcbias, calc_frag
    gcbias_python: "/hwfssz8/MGI_CG_SZ/USER/luozhenyu/anaconda3/envs/gcbias/bin/python"
    ########### ###########

    # choose from 'se_bc5'=bc at 5'of read2, 'standard_pe', 'standard_se', 'bc_start'=bc at 5' of R2s, 'skip_bc1', 'diff_length'=PE40+200 (SPHY12AA/SPHY20AA:R1=40gDNA+R2=42BC+45adapter+113gDNA, or or R1=40gDNA+R2=42BC+79adapter+79gDNA), 'random_bc'
    bc_condition: 'diff_length'
    bwa_mem: 'mem'
    gc_swap: False
    platform: "BGI-seq"
    # Read length without barcode (SE treate as R2, PE, R2 for PE diff_length), adapter+gDNA: 79+79 or 45+113 for PE40+200, 9 for SE51 
    read_len: 113
    adapter_len: 45
    read1_len: 40
    bc_len : 42
    barcode: "/barcode.list"
    barcode_RC: "/barcode_RC.list"
    # sentieon install directory. This is specific to their commercial tools, not the assembly software
    sentieon_install: "/opt/sentieon-genomics-201808.07"
    # License server path
    sentieon_license: "SENTIEON_LICENSE=sentieon-license-server.completegenomics.com:8990"
    # Path to the python version Wenlan used for some analyses
    wenlan_python: "/opt/cgi-anaconda2-4.4.0/bin/python"

    
    # Path to Sentieon's variant filtering model
    sentieon_model: "/research/rv-02/home/eanderson/CGI_WGS_Pipeline/Data_and_Tools/data/DNAscope_BGI_model_beta.txt"
    # bcftools install
    bcftools: "/opt/cgi-tools/bin/bcftools"
    # a parameter for hapcut, shouldn't have to be changed
    hapcut_link_dist: 100000
    # LongHap executable path
    longhap: "/home/ysun/LongHap_v1.3.1/Bin/LongHap.pl"

calc_frag:
    # split distances to use for long fragment calculations
    split_dist: [50000, 300000]
    # minimum fragment length for fragment calculations
    min_frag: 750
    # true=check dup pos, but N_Reads always dedup; include duplicates, in fragment calculations. If two reads share the same barcode and mapping position they're treated as duplicates anyway.
    include_dups: True
    # object_store_memory=200*1024*1024*1024, memory=100*1024*1024*1024 [(200,1024), (100, 1024)]
    # set ray mem, no space between numbers, need to be 200G
    ray_mem: "200,1024,100,1024"
    # ray_mem: "9000,1,9000,1"
    writeouttsvs: True
longhap:
    # Longhap parameters, window size determines bin size for calculations
    win_size: 10000000
    # Max expected length of LFR fragment
    max_lfr_len: 300000
    min_barcode: 1
    min_link: 1
benchmark:
    ############ path to snp benchmark in SJ ############
    # # Should just be able to change the number in HG00N to the appropriate sample
    # benchmark_snp: "/research/rv-02/home/eanderson/CGI_WGS_Pipeline/Data_and_Tools/data/hg38_GIAB_HG001/benchmark_snp.chrfix.vcf.gz"
    # # path to indel benchmark
    # benchmark_indel: "/research/rv-02/home/eanderson/CGI_WGS_Pipeline/Data_and_Tools/data/hg38_GIAB_HG001/benchmark_indel.chrfix.vcf.gz"
    # # path to high confidence bed file
    # bedfile: "/research/rv-02/home/eanderson/CGI_WGS_Pipeline/Data_and_Tools/data/hg38_GIAB_HG001/benchmark_bed.chrfix.bed"
    # # path to reference sdf, should just need to change if you change the reference and plan to benchmark
    # ref_sdf: "/research/rv-02/home/eanderson/CGI_WGS_Pipeline/Data_and_Tools/data/hg38_benchmark_ref/GRCh38.sdf"
    ############ ############


    ############ path to snp benchmark in SZ ############
    # Should just be able to change the number in HG00N to the appropriate sample
    benchmark_snp: "/home/ycai/ycai/pipeline/CGI_WGS_Pipeline/Data_and_Tools/data/hg38_GIAB_HG001/benchmark_snp.chrfix.vcf.gz"
    # path to indel benchmark
    benchmark_indel: "/home/ycai/ycai/pipeline/CGI_WGS_Pipeline/Data_and_Tools/data/hg38_GIAB_HG001/benchmark_indel.chrfix.vcf.gz"
    # path to high confidence bed file
    bedfile: "/home/ycai/ycai/pipeline/CGI_WGS_Pipeline/Data_and_Tools/data/hg38_GIAB_HG001/benchmark_bed.chrfix.bed"
    # path to reference sdf, should just need to change if you change the reference and plan to benchmark
    ref_sdf: "/home/ycai/ycai/pipeline/CGI_WGS_Pipeline/Data_and_Tools/data/hg38_benchmark_ref/GRCh38.sdf"
    ############ ############
    
    # leave blank unless it's a benchmarked sample
    # Currently these directories aren't set up for hg38, though they're really just the truth VCFs split up by chromosome
    truth_vcf_dir: "/home/ycai/hm/dev/pipeline/by_eanderson/CGI_WGS_Pipeline/Data_and_Tools/data/truth_vcf_dir" # directorty of vcfs for phasing; formatted as truth_chroms_{chr}.vcf
threads:
    # threads for various multi-threaded processes
    # These can be modified if you wish
    # They're automatically capped to the number of threads you specify when running snakemake with the -j option
    ### # in SJ
    # bwa: 100
    # haplotyper: 100
    ########### in SZ ############
    bwa: 20
    haplotyper: 20
    ############ ############

    metrics: 10
    gnu_parallel: 20
    calc_frag: 4
